# Как работает модель классификации текста

## Обзор

Модель использует **semi-supervised learning** (полуобучение с учителем) для классификации текстов по двум задачам:
1. **Тональность** (sentiment): `positive` / `negative`
2. **Стиль** (style): `colloquial` / `journalistic` / `literary`

Используется **нейронная сеть на PyTorch** с подходом **pseudo-labeling** для semi-supervised learning.

## Архитектура модели

Модель состоит из следующих компонентов:

```
Текст → Токенизация → Embedding → Global Average Pooling → Dense → Dropout → Выход
```

---

## 1. Токенизация и создание словаря

### Что это такое?

Перед обучением нейросети нужно преобразовать тексты в числа, которые может понимать компьютер.

### Как это работает?

#### Шаг 1: Создание словаря

Собираем все уникальные слова из всех текстов (размеченных и неразмеченных) и создаём словарь:

```
Словарь:
"отличный" → 1
"фильм" → 2
"ужасное" → 3
"обслуживание" → 4
...
"<UNK>" → 0  (неизвестные слова)
```

#### Шаг 2: Преобразование текста в последовательность чисел

```
"Это отличный фильм!" 
  ↓
[0, 1, 2, 0]  (индексы слов в словаре)
```

#### Шаг 3: Дополнение до одинаковой длины

Все последовательности должны быть одинаковой длины (MAX_LEN = 50):

```
[0, 1, 2, 0, 0, 0, ..., 0]  (50 чисел)
```

---

## 2. Embedding слой

### Что это такое?

**Embedding** — это слой нейросети, который преобразует индексы слов в плотные векторы (векторы чисел).

### Как это работает?

#### До Embedding:
```
Слово "отличный" → индекс 1 → [1]
```

#### После Embedding:
```
Слово "отличный" → индекс 1 → [0.3, -0.5, 0.8, ..., 0.2]  (128 чисел)
```

Каждое слово превращается в вектор из 128 чисел. Эти числа обучаются во время обучения модели.

**Почему это важно?**
- Похожие слова получают похожие векторы
- Модель может понимать семантику слов
- Векторы содержат информацию о значении слова

### Параметры в нашем коде

```python
Embedding(
    input_dim=vocab_size,    # Размер словаря (например, 305 слов)
    output_dim=128,          # Размерность вектора для каждого слова
)
```

**Итого:** 305 × 128 = 39,040 параметров только для embedding!

---

## 3. Global Average Pooling

### Что это такое?

**Global Average Pooling** — это операция, которая усредняет все векторы слов в тексте.

### Как это работает?

#### До Pooling:
```
Текст: "Это отличный фильм"
  ↓ Embedding
[вектор_это, вектор_отличный, вектор_фильм]  (3 вектора по 128 чисел)
```

#### После Pooling:
```
Усредняем все векторы:
(вектор_это + вектор_отличный + вектор_фильм) / 3
  ↓
[0.2, -0.1, 0.5, ..., 0.3]  (1 вектор из 128 чисел)
```

**Почему это работает?**
- Получаем одно представление всего текста
- Проще чем LSTM для маленьких данных
- Учитывает все слова в тексте

---

## 4. Dense (полносвязные) слои

### Что это такое?

**Dense слои** (полносвязные слои) — это слои, где каждый нейрон соединён со всеми нейронами предыдущего слоя.

### Как это работает?

#### Слой 1: Dense(128 → 32)
```
Вход: [0.2, -0.1, 0.5, ..., 0.3]  (128 чисел)
  ↓
Каждое число умножается на веса и суммируется
  ↓
Выход: [0.7, -0.3, 0.4, ..., 0.1]  (32 числа)
```

**Математически:**
```
output = ReLU(W × input + b)
```

Где:
- `W` — матрица весов (128 × 32 = 4,096 параметров)
- `b` — вектор смещений (32 параметра)
- `ReLU` — функция активации (max(0, x))

#### Слой 2: Dense(32 → 2-3)
```
Вход: [0.7, -0.3, 0.4, ..., 0.1]  (32 числа)
  ↓
Выход: [2.3, -1.5]  (2 числа для sentiment) или [1.2, -0.5, 0.8] (3 числа для style)
```

Эти числа называются **логитами** (logits) — сырые предсказания до применения softmax.

---

## 5. Dropout (регуляризация)

### Что это такое?

**Dropout** — это техника регуляризации, которая случайно "отключает" 30% нейронов во время обучения.

### Как это работает?

```
Без Dropout:
[0.7, -0.3, 0.4, 0.1, 0.9, ...]  (все нейроны активны)

С Dropout (30%):
[0.7, 0, 0.4, 0, 0.9, ...]  (30% нейронов отключены)
```

**Почему это нужно?**
- Предотвращает переобучение (overfitting)
- Заставляет модель быть более устойчивой
- Модель не полагается на конкретные нейроны

**Важно:** Dropout работает только во время обучения! Во время предсказания все нейроны активны.

---

## 6. Softmax и предсказание

### Что это такое?

**Softmax** преобразует логиты в вероятности классов.

### Как это работает?

#### До Softmax:
```
Логиты: [2.3, -1.5]
```

#### После Softmax:
```
Вероятности: [0.92, 0.08]
```

Это означает:
- Класс 0 (positive): 92% вероятность
- Класс 1 (negative): 8% вероятность

**Предсказание:** выбираем класс с максимальной вероятностью → `positive`

### Формула Softmax:

```
softmax(x_i) = exp(x_i) / sum(exp(x_j)) для всех j
```

---

## 7. Полная архитектура нейронной сети

```
Входной текст: "Это отличный фильм!"
    ↓
1. Токенизация
   → [0, 1, 2, 0, 0, ..., 0]  (50 чисел)
    ↓
2. Embedding (305 слов × 128 размерность)
   → [[0.1, -0.2, ...], [0.3, 0.5, ...], ...]  (50 векторов по 128 чисел)
    ↓
3. Global Average Pooling
   → [0.2, -0.1, 0.5, ..., 0.3]  (1 вектор из 128 чисел)
    ↓
4. Dense(128 → 32) + ReLU
   → [0.7, -0.3, 0.4, ..., 0.1]  (32 числа)
    ↓
5. Dropout(30%)
   → [0.7, 0, 0.4, 0, ..., 0.1]  (30% отключены)
    ↓
6. Dense(32 → 2) для sentiment
   → [2.3, -1.5]  (логиты)
    ↓
7. Softmax
   → [0.92, 0.08]  (вероятности)
    ↓
Предсказание: "positive" (92% уверенность)
```

**Итого параметров:**
- Embedding: 305 × 128 = 39,040
- Dense 1: 128 × 32 + 32 = 4,128
- Dense 2: 32 × 2 + 2 = 66
- **Всего: ~43,234 параметра**

---

## 8. Semi-supervised Learning (Pseudo-labeling)

### Что это такое?

**Pseudo-labeling** — это метод semi-supervised learning, который:
1. Обучает модель на размеченных данных
2. Предсказывает метки для неразмеченных данных
3. Берёт только высокоуверенные предсказания (порог 0.85)
4. Переобучает модель на размеченных + псевдо-размеченных данных

### Как это работает?

#### Этап 1: Обучение на размеченных данных

```
Размеченные данные (84 примера):
"Это отличный фильм!" → positive
"Ужасное обслуживание" → negative
...

Модель обучается 100-200 эпох
Точность растёт: 0.5 → 0.6 → 0.7 → ... → 0.98
```

#### Этап 2: Pseudo-labeling

```
Неразмеченные данные (62 примера):
"Фильм оказался интересным"
"Покупка была удачной"
...

Модель предсказывает метки:
"Фильм оказался интересным" → positive (уверенность: 0.92) ✓
"Покупка была удачной" → positive (уверенность: 0.65) ✗ (низкая уверенность)
...

Берём только высокоуверенные (≥ 0.85):
Найдено 56 примеров с уверенностью ≥ 0.85
```

#### Этап 3: Переобучение

```
Объединяем данные:
Размеченные: 84 примера
Псевдо-размеченные: 56 примеров (только высокоуверенные)
Итого: 140 примеров

Переобучаем модель с меньшим learning rate (×0.1)
Меньше эпох (EPOCHS // 3)
```

**Почему это работает?**
- Используем больше данных для обучения
- Высокоуверенные предсказания обычно правильные
- Меньший learning rate предотвращает переобучение

**Осторожность:**
- Если порог уверенности слишком низкий, модель может переобучиться на неправильных метках
- Поэтому используем высокий порог (0.85) и ограничиваем количество псевдо-меток

---

## 9. Процесс обучения

### Этап 1: Загрузка данных

```python
# Размеченные данные (84 примера)
X_labeled = ["Это отличный фильм!", "Ужасное обслуживание.", ...]
y_labeled = ["positive", "negative", ...]

# Неразмеченные данные (62 примера)
X_unlabeled = ["Фильм оказался интересным.", "Покупка была удачной.", ...]
```

### Этап 2: Создание словаря

```python
# Собираем все слова из всех текстов
word_to_idx = {
    "<UNK>": 0,
    "отличный": 1,
    "фильм": 2,
    "ужасное": 3,
    ...
}
vocab_size = 305
```

### Этап 3: Создание нейронной сети

```python
model = TextClassifier(
    vocab_size=305,
    embedding_dim=128,
    num_classes=2  # для sentiment
)
```

### Этап 4: Обучение на размеченных данных

```python
for epoch in range(200):
    for batch in data_loader:
        # Прямой проход
        outputs = model(sequences)
        loss = criterion(outputs, labels)
        
        # Обратный проход (backpropagation)
        loss.backward()
        optimizer.step()
    
    # Проверяем точность
    accuracy = evaluate(model, labeled_data)
    print(f"Epoch {epoch}, Accuracy: {accuracy}")
```

**Что происходит:**
- Модель предсказывает метки
- Сравнивает с реальными метками
- Вычисляет ошибку (loss)
- Обновляет веса через backpropagation
- Точность растёт с каждой эпохой

### Этап 5: Pseudo-labeling

```python
# Предсказываем метки для неразмеченных данных
predictions = model.predict(unlabeled_data)
confident_predictions = filter_by_confidence(predictions, threshold=0.85)

# Переобучаем на объединённых данных
combined_data = labeled_data + confident_predictions
model.fine_tune(combined_data, learning_rate=0.0001)
```

### Этап 6: Оценка качества

```python
# Предсказываем на размеченных данных
y_pred = model.predict(X_labeled)
accuracy = accuracy_score(y_labeled, y_pred)

# Выводим отчёт
print(classification_report(y_labeled, y_pred))
```

---

## 10. Процесс предсказания

### Когда модель обучена, как она предсказывает новый текст?

```
Новый текст: "Отличный фильм!"
    ↓
1. Токенизация
   → [0, 1, 2, 0, 0, ..., 0]
    ↓
2. Embedding
   → [[0.1, -0.2, ...], [0.3, 0.5, ...], ...]
    ↓
3. Global Average Pooling
   → [0.2, -0.1, 0.5, ..., 0.3]
    ↓
4. Dense слои
   → [2.3, -1.5]
    ↓
5. Softmax
   → [0.92, 0.08]
    ↓
6. Argmax (выбираем максимальную вероятность)
   → 0 (positive)
    ↓
7. Декодирование через LabelEncoder
   → "positive"
    ↓
Результат: "positive"
```

---

## 11. Параметры модели

### Что такое параметры?

**Параметры** — это числа (веса и смещения), которые модель учит во время обучения. Это "память" модели, которая хранит всё, что она узнала о данных.

**Параметры бывают двух типов:**
1. **Веса (weights)** — связи между нейронами
2. **Смещения (bias)** — дополнительные числа, добавляемые к выходам

### Откуда берутся параметры в нашей модели?

#### 1. Embedding слой: ~39,040 параметров (90% всех параметров!)

```python
Embedding(vocab_size=305, embedding_dim=128)
```

**Что это:**
- Для каждого слова в словаре создаётся вектор из 128 чисел
- Эти числа обучаются во время обучения

**Расчёт:**
```
305 слов × 128 чисел = 39,040 параметров
```

**Пример:**
```
Слово "отличный" (индекс 1):
  → [0.3, -0.5, 0.8, 0.1, ..., 0.2]  (128 чисел)

Слово "фильм" (индекс 2):
  → [-0.2, 0.4, 0.6, -0.3, ..., 0.1]  (128 чисел)

Каждое из этих 128 чисел — отдельный параметр!
```

**Почему так много:**
- У нас 305 уникальных слов в словаре
- Каждое слово представлено вектором из 128 чисел
- 305 × 128 = 39,040 параметров только для embedding!

**Это самый большой источник параметров** — около 90% всех параметров модели.

#### 2. Dense слой 1 (128 → 32): ~4,128 параметров

```python
Linear(embedding_dim=128, out_features=32)
```

**Что это:**
- Матрица весов W: 128 входных нейронов × 32 выходных нейрона
- Вектор смещений b: по одному на каждый выходной нейрон

**Расчёт:**
```
Матрица весов: 128 × 32 = 4,096 параметров
Вектор смещений: 32 параметра
Итого: 4,096 + 32 = 4,128 параметров
```

**Как работает:**
```
Вход: [x1, x2, ..., x128]  (128 чисел)
  ↓
Умножение на матрицу весов:
  [x1, x2, ..., x128] × W[128×32] = [y1, y2, ..., y32]
  ↓
Добавление смещений:
  [y1, y2, ..., y32] + [b1, b2, ..., b32] = [z1, z2, ..., z32]
  ↓
Выход: [z1, z2, ..., z32]  (32 числа)
```

**Каждое число в матрице W и векторе b — отдельный параметр.**

#### 3. Dense слой 2 (32 → 2-3): ~66-99 параметров

```python
Linear(32, num_classes)  # num_classes = 2 для sentiment, 3 для style
```

**Для sentiment (2 класса):**
```
Матрица весов: 32 × 2 = 64 параметра
Вектор смещений: 2 параметра
Итого: 64 + 2 = 66 параметров
```

**Для style (3 класса):**
```
Матрица весов: 32 × 3 = 96 параметров
Вектор смещений: 3 параметра
Итого: 96 + 3 = 99 параметров
```

### Итого параметров:

**Для sentiment:**
- Embedding: 39,040 параметров (90.3%)
- Dense 1: 4,128 параметров (9.5%)
- Dense 2: 66 параметров (0.2%)
- **Всего: 43,234 параметра**

**Для style:**
- Embedding: 40,064 параметра (90.5%)
- Dense 1: 4,128 параметров (9.3%)
- Dense 2: 99 параметров (0.2%)
- **Всего: 44,291 параметр**

### Почему так много параметров?

#### 1. Embedding — основной источник (90%!)

**Почему embedding даёт так много параметров:**
- У нас 305 уникальных слов в словаре
- Каждое слово представлено вектором из 128 чисел
- 305 × 128 = 39,040 параметров

**Это необходимо, потому что:**
- Каждое слово должно иметь своё уникальное представление
- Векторы слов обучаются, чтобы похожие слова были близки в пространстве
- 128 размерность позволяет хранить достаточно информации о семантике слова

#### 2. Dense слои — полносвязные соединения

**Почему Dense слои дают много параметров:**
- Каждое соединение между нейронами = 1 параметр
- 128 входных нейронов × 32 выходных нейрона = 4,096 соединений
- Плюс 32 смещения = 4,128 параметров

**Это необходимо, потому что:**
- Модель должна научиться комбинировать признаки из embedding
- Каждое соединение — это "правило", которое модель учит
- Больше соединений = больше возможностей для обучения паттернов

### Можно ли уменьшить количество параметров?

Да, можно, но это может ухудшить качество:

#### 1. Уменьшить размер словаря

**Было:**
```
305 слов × 128 = 39,040 параметров
```

**Станет (если оставить только 200 самых частых слов):**
```
200 слов × 128 = 25,600 параметров
Экономия: -13,440 параметров
```

**Проблема:** Модель не сможет понимать редкие слова.

#### 2. Уменьшить embedding размерность

**Было:**
```
305 слов × 128 = 39,040 параметров
```

**Станет (если уменьшить до 64):**
```
305 слов × 64 = 19,520 параметров
Экономия: -19,520 параметров
```

**Проблема:** Меньше информации о каждом слове, хуже качество.

#### 3. Уменьшить размер Dense слоя

**Было:**
```
128 × 32 + 32 = 4,128 параметров
```

**Станет (если уменьшить до 16 нейронов):**
```
128 × 16 + 16 = 2,064 параметра
Экономия: -2,064 параметра
```

**Проблема:** Меньше возможностей для извлечения паттернов.

### Зачем столько параметров?

**Параметры хранят знания модели:**

1. **Embedding параметры:**
   - Семантика слов (похожие слова → похожие векторы)
   - Контекстное значение слов
   - Связи между словами

2. **Dense 1 параметры:**
   - Паттерны в текстах
   - Как комбинировать признаки из embedding
   - Какие слова важны для классификации

3. **Dense 2 параметры:**
   - Как комбинировать паттерны для предсказания класса
   - Веса для каждого класса

**Больше параметров = больше "памяти"**, но:
- Нужно больше данных для обучения
- Может переобучаться на маленьких датасетах
- Требует больше вычислительных ресурсов

### Для нашего датасета (84 примера):

**43,000 параметров — это много!**

- Соотношение: 43,000 параметров / 84 примера ≈ 512 параметров на пример
- Рекомендуется: 10-100 параметров на пример
- **Наш случай:** В 5-50 раз больше, чем рекомендуется!

**Поэтому:**
- Используем Dropout (30%) для регуляризации
- Используем Early Stopping
- Ограничиваем количество pseudo-labels
- Упростили архитектуру (убрали LSTM)

**Если бы было больше данных (500+ примеров), можно было бы:**
- Увеличить embedding размерность (до 256)
- Добавить больше Dense слоёв
- Использовать LSTM

### Гиперпараметры (настраиваются вручную):

- **MAX_WORDS = 1000**: Максимальный размер словаря
- **MAX_LEN = 50**: Максимальная длина последовательности
- **EMBEDDING_DIM = 128**: Размерность embedding векторов
- **BATCH_SIZE = 4**: Размер батча для обучения
- **EPOCHS = 200**: Максимальное количество эпох
- **LEARNING_RATE = 0.001**: Скорость обучения
- **Dropout = 0.3**: Процент отключаемых нейронов (30%)
- **Confidence threshold = 0.85**: Порог уверенности для pseudo-labeling

---

## 12. Преимущества и недостатки

### Преимущества

- **Использует неразмеченные данные** — может улучшить качество при малом количестве размеченных данных
- **Нейронная сеть** — может улавливать сложные паттерны
- **Embedding** — понимает семантику слов
- **Простая архитектура** — легко понять и объяснить студентам

### Недостатки

- **Требует много данных** — для хорошего качества нужно 100+ примеров на класс
- **Может переобучаться** — особенно на маленьких датасетах
- **Pseudo-labeling может ухудшить качество** — если порог уверенности слишком низкий
- **Вычислительно затратно** — обучение занимает время

---

## 13. Почему модель может ошибаться?

### Проблема 1: Мало данных

**84 примера на 2 класса — это всё ещё мало!**

- Модель не видит достаточно разнообразия
- Может переобучаться на обучающих данных
- Плохо обобщается на новые данные

**Решение:** Добавить больше примеров (100+ на класс).

### Проблема 2: Несбалансированные классы

Если в данных больше примеров одного класса, модель может "склоняться" к нему.

**Пример:**
- 60 примеров `positive`
- 24 примера `negative`

→ Модель будет чаще предсказывать `positive`.

**Решение:** Использовать веса классов в loss function (уже реализовано).

### Проблема 3: Неправильные pseudo-labels

Если порог уверенности слишком низкий, модель может переобучиться на неправильных псевдо-метках.

**Решение:** Использовать высокий порог (0.85) и ограничивать количество псевдо-меток.

### Проблема 4: Переобучение

Модель запоминает обучающие данные, но плохо работает на новых.

**Признаки:**
- Высокая точность на обучающих данных (0.98)
- Низкая точность на новых данных (0.50)

**Решение:** 
- Больше Dropout
- Больше данных
- Меньше параметров модели

---

## 14. Как улучшить модель?

### 1. Добавить больше данных

```csv
# В data/sentiment_labeled.csv добавить:
"Мне очень понравилось!",positive
"Не рекомендую.",negative
...
```

### 2. Использовать более продвинутую архитектуру

- **LSTM** — для учёта порядка слов (но сложнее для маленьких данных)
- **BERT** — предобученная модель (очень мощно, но требует много данных)
- **CNN** — свёрточные слои для текста

### 3. Настроить гиперпараметры

Попробовать разные значения:
```python
LEARNING_RATE = [0.0001, 0.001, 0.01]
EMBEDDING_DIM = [64, 128, 256]
Dropout = [0.2, 0.3, 0.5]
```

### 4. Использовать предобученные embeddings

Вместо обучения embedding с нуля, использовать предобученные (Word2Vec, FastText, GloVe).

---

## 15. Визуализация процесса

```
┌─────────────────────────────────────────────────────────┐
│                    ОБУЧЕНИЕ МОДЕЛИ                       │
└─────────────────────────────────────────────────────────┘

1. ЗАГРУЗКА ДАННЫХ
   ┌──────────────┐    ┌──────────────┐
   │ Размеченные  │    │ Неразмеченные│
   │ (84 примера) │    │ (62 примера) │
   └──────────────┘    └──────────────┘

2. СОЗДАНИЕ СЛОВАРЯ
   Все слова → Индексы
   "отличный" → 1
   "фильм" → 2
   ...

3. СОЗДАНИЕ НЕЙРОННОЙ СЕТИ
   Embedding(305, 128)
     ↓
   Global Average Pooling
     ↓
   Dense(128 → 32) + ReLU
     ↓
   Dropout(30%)
     ↓
   Dense(32 → 2) + Softmax

4. ОБУЧЕНИЕ НА РАЗМЕЧЕННЫХ ДАННЫХ
   Epoch 1:  Accuracy: 0.50
   Epoch 10: Accuracy: 0.60
   Epoch 50: Accuracy: 0.90
   Epoch 100: Accuracy: 0.98
   ...

5. PSEUDO-LABELING
   Неразмеченные данные
     ↓
   Предсказания (уверенность ≥ 0.85)
     ↓
   56 высокоуверенных примеров

6. ПЕРЕОБУЧЕНИЕ
   Размеченные (84) + Псевдо-размеченные (56) = 140 примеров
     ↓
   Fine-tuning с меньшим learning rate

7. СОХРАНЕНИЕ
   ┌──────────────┐    ┌──────────────┐    ┌──────────────┐
   │   Модель     │    │   Словарь     │    │   Encoder    │
   │  (.pth файл) │    │  (.pkl файл)  │    │  (.pkl файл)  │
   └──────────────┘    └──────────────┘    └──────────────┘

┌─────────────────────────────────────────────────────────┐
│                  ПРЕДСКАЗАНИЕ                            │
└─────────────────────────────────────────────────────────┘

Новый текст: "Отличный фильм!"
      ↓
1. Токенизация → [0, 1, 2, 0, ...]
      ↓
2. Embedding → векторы слов
      ↓
3. Global Average Pooling → один вектор
      ↓
4. Dense слои → логиты
      ↓
5. Softmax → вероятности [0.92, 0.08]
      ↓
6. Argmax → класс 0
      ↓
7. Декодирование → "positive"
      ↓
Результат: "positive"
```

---

## 16. Сравнение: Наша модель vs Другие подходы

| Параметр | Наша модель (NN + Pseudo-labeling) | Label Spreading | BERT |
|----------|-----------------------------------|-----------------|------|
| **Тип** | Нейронная сеть | Алгоритм на графах | Трансформер |
| **Слои** | 4 слоя (Embedding, Pooling, 2×Dense) | Нет | 12+ слоёв |
| **Параметры** | ~43,000 | Граф связей | 110M+ |
| **Требует данных** | Средне (50+ примеров) | Мало (20+ примеров) | Много (1000+ примеров) |
| **Скорость обучения** | Средняя (минуты) | Быстрая (секунды) | Медленная (часы) |
| **Качество** | Хорошее на средних данных | Среднее на маленьких данных | Отличное на больших данных |
| **Объяснимость** | Средняя | Высокая | Низкая |

**Наша модель:** Хороший баланс между сложностью и качеством для обучения студентов.

---

## Заключение

Модель использует **нейронную сеть на PyTorch** с **semi-supervised learning** (pseudo-labeling) для классификации текстов:

1. **Токенизация** преобразует тексты в последовательности чисел
2. **Embedding** создаёт векторные представления слов
3. **Global Average Pooling** усредняет векторы слов
4. **Dense слои** извлекают признаки и делают предсказания
5. **Pseudo-labeling** использует неразмеченные данные для улучшения модели

**Архитектура:**
- 4 слоя
- ~43,000 параметров
- Работает на средних датасетах (50-100 примеров на класс)

Для улучшения качества нужно:
- Добавить больше размеченных данных (100+ примеров на класс)
- Попробовать другие архитектуры (LSTM, BERT)
- Настроить гиперпараметры

**Для обучения студентов это отличный пример:**
- Показывает работу нейронных сетей
- Демонстрирует semi-supervised learning
- Легко понять архитектуру
- Можно экспериментировать с параметрами
